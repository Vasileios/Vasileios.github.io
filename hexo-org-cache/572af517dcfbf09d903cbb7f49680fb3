{"md5":"ad4611216e1b53ef2b8633f63f458ed2","content":"<div id=\"content\">\n\n\n<div id=\"outline-container-orgf42e8ce\" class=\"outline-2\">\n<h2 id=\"orgf42e8ce\">Sonification case study</h2>\n<div class=\"outline-text-2\" id=\"text-orgf42e8ce\">\n<p>\nSonifcation of scientific  data has progressed over the last 2 decades. This attempts to change our aural perception and train our ears and brain in a way to understand the information of specific events or phenomena and make a better understanding of the complexity of scientific data. Especially in large data exploration which contains much more information than this we can display visually. ”the auditory system is the best pattern-recognition device that we know of,” says Bruce Walker, a professor of psychology and director of the Georgia Institute of Technology’s Sonification Lab. ”If you’re looking through a data set and trying to understand what’s going on, it’s o en easier and more eficient to listen to the sound of it rather than looking at a screen or a printed version.”1 Diaz Merced, and Wanda Liz (2013) indicate in their thesis with the title ”Sound for the exploration of space physics data” that:\nlarge data sets have a large amount of information spanning dozens of orders of magnitude in space and time. It is also important to consider the limitations improved by nature on the human eye. In addition, even the best computer screens available are limited to a range of spatial resolution.  is limitation a ects the useful dynamic range of the display, reducing the amount of data scientists can study at any one time.(Merced, Wanda, 2013. p.7)\n</p>\n</div>\n</div>\n\n\n<div id=\"outline-container-org1cb8750\" class=\"outline-2\">\n<h2 id=\"org1cb8750\">Auditory dimension</h2>\n<div class=\"outline-text-2\" id=\"text-org1cb8750\">\n<ul class=\"org-ul\">\n<li>What data and how to map.(Gaël Dubus* and Roberto Bresin 2013)</li>\n<li>the use of mappings and metaphors in auditory displays.(Walker, Kramer 1996 )</li>\n<li>transform inaudible data into audible information</li>\n<li>3D interactive sound</li>\n</ul>\n</div>\n</div>\n\n<div id=\"outline-container-org40c49a5\" class=\"outline-2\">\n<h2 id=\"org40c49a5\">State of the art</h2>\n<div class=\"outline-text-2\" id=\"text-org40c49a5\">\n<p>\nArthur Paté Lapo Boschi, Jean-Loïc Le Carrou,Benjamin Holtzman. Categorization of seismic sources by auditory display: A blind test.International Journal of Human-Computer Studies. 2016\nMichele Geronazzo Alberto Bedin, Luca Brayda Claudio Campus Federico Avanzini. Interactive spatial soni cation for non-visual exploration of virtual maps. International Journal of Human-Computer Studies. 2016 (<a href=\"http://www.sciencedirect.com/science/article/pii/\">http://www.sciencedirect.com/science/article/pii/</a> S1071581915001287)\nSergio Masce i, Lorenzo Picinali, Andrea Gerino, Dragan Ahmetovic, Cristian Bernareggi .Soni cation of guidance data during road crossing for people with visual impairments or blindness. International Journal of Human-Computer Studies. 2016\nSandra Paule o,(Guest Editor), Howard Cambridge, (Guest Editor), Patrick Susini. Data soni cation and sound design in interactive systems. International Journal of Human-Computer Studies. 2016\n</p>\n</div>\n</div>\n<div id=\"outline-container-org30af38c\" class=\"outline-2\">\n<h2 id=\"org30af38c\">Other Sonifcation examples</h2>\n<div class=\"outline-text-2\" id=\"text-org30af38c\">\n<p>\nFrom: <a href=\"http://www.open-shelf.ca/\">http://www.open-shelf.ca/</a> 160201-data-sonification/\nLARAS Sonifcation Laboratory’s projects that allow network administrators to monitor network activity by listening to soni cations.\nSonifcation has also been used for real-time monitoring of athletes’ physical performance. Various sensors collect information about athletes’ motions, posture, and force, then that information is sonified so athlete’s can listen to their performance in real-time. For example, Stephan Barrass described a workshop he led about using sonification for elite rowing training. Athletes using these sort of applications can listen to feedback about their performance without needing to compromising their visual focus.\n e LHC Sound project: LHC (Large Hardon Collider) Sound so ware engineer Archer Enrich said the sonification is “true to the data, and it’s telling you something about the data that you couldn’t know in any other way.” He also noted that when listening to the sonification, “you feel closer to the mystery of Nature which I think a lot of scientists do when they get deep into these ma ers.”\nBrian Foo’s sonification of income inequality in New York City provides an example ”the sonification is entertaining simply as a piece of music and manages to powerful information about income inequality in a few short minutes.”\nWanda Diaz Merced describes how losing her sight led her to investigate new ways of studying space physics, using sound rather than visual information.(<a href=\"https://www.nasa.gov/centers/goddard/about/people/Wanda_Diaz-Merced.html\">https://www.nasa.gov/centers/goddard/about/people/Wanda_Diaz-Merced.html</a>)\nHeavenly Sounds: Hearing Astronomical Data Can Lead to Scientific\nInsights Converting the energetic hail of cosmic radiation into audible\ntracks has produced be er understanding of the solar wind and other\nastrophysical events—along with musical enjoyment: COSMIC SYMPHONICS:\n e ”solar wind,” a barrage of charged particles streaming from the sun\n(red arrows; yellow represents magnetic  eld lines), is one example of an\nastrophysical phenomenon that can be translated into audible sound. Credit:\nESA/NASA <a href=\"https://www.scientificamerican.com/article/heavenly-sounds-hearing-astronomical-data-can-lead-to-scientific-Listening\">https://www.scientificamerican.com/article/heavenly-sounds-hearing-astronomical-data-can-lead-to-scientific-Listening</a> to Solar Storms | MconneX | MichEpedia: <a href=\"https://www.youtube.com/watch?v=S-saaAyaW0c&amp;feature=youtu.be\">https://www.youtube.com/watch?v=S-saaAyaW0c&amp;feature=youtu.be</a>\nSounds of Saturn - NASA Voyager Recording: <a href=\"https://www.youtube.com/watch?v=X_JAvVjKeWI\">https://www.youtube.com/watch?v=X_JAvVjKeWI</a>\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-orga70d702\" class=\"outline-2\">\n<h2 id=\"orga70d702\">Sonifcation Techniques and approaches</h2>\n<div class=\"outline-text-2\" id=\"text-orga70d702\">\n<p>\nData exploration\nthe first thing we need to consider is to explore-investigate and  nally choose the data which can be used both for scienti c and artistic purposes.\n</p>\n\n<p>\nAdditionally, the base of soni cation visualisation will be some combination of passive listening and active listener interaction.\nOur plan is to use iPython tools and /CSV File Readers/2 to import data, then through OSC we will send the data to other programming languages such as SuperCollider. In SuperCollider we will use ’Pa ern style’ mapping techniques. Moreover, it is important to have a speci c sound dimension (20Hz-20KHz) to represent a given data dimension. To achieve this, we use parameter mapping soni cation3 and model based soni cation techniques4.  is action, will be repeating on other sound models and synthesis techniques such as additive, Fourier sinusoidal synthesis, fm, granular among others. Additionally, we need to choose an appropriate polarity for the data. For example, if in meteorological data there is an increasing of the temperature we need to increase the frequency and or the volume of the sound and the opposite (positive and negative mapping polarity, Walker, 2002). Considering the di erences between visual and auditory perception it is important not to have as a guide always the visual perception. For more information see here5 Furthermore, a three dimensional visual representation of data is accompanied from an analogous auditory spatial impression. In this case, we are interested in experimenting with spatialisation in both multichannel and stereo representations. To achieve a 3D immersive experience for both soni cation and visualisation purposes we chose to use game engines such as Unreal Engine 4.\n</p>\n</div>\n</div>\n\n\n<div id=\"outline-container-orge450bec\" class=\"outline-2\">\n<h2 id=\"orge450bec\">Sonification of magnetic storm</h2>\n<div class=\"outline-text-2\" id=\"text-orge450bec\">\n<p>\nBelow is an demonstration of the technique in <b>SuperCollider</b>. With this technique we are able to read and sonify data from <b>NOA's</b> magnetometer. This example also contains the code for sending <b>osc</b> messages to other applications such as <b>openFrameworks</b> and <b>Cinder</b> for visualisation purposes.\n</p>\n\n<pre class=\"example\">\n// =====================================================================\n// SuperCollider Workspace\n// =====================================================================\n//: Data path\ns.boot;\n(\n~files = \"./data/MagneticStorm12-15\\ March2016_NOA\\'s\\ magnetometer/*.dat.txt\".pathMatch;\n)\n//:load and collect data\n(\n~load = { | path |\n\tvar data;\n\t// select only these rows which contain 7 columns exactly:\n\tdata = CSVFileReader.read(path) select: { | row, column |\n\t\trow.size == 7;\n\t\t//column.size == 10;\n\t};\n\tdata.flop[2..4].flop collect: { | row |\n\t\trow collect: { | string |\n\t\t\tstring.replace(\"+\", \"\").interpret;\n\t\t}\n\t};\n};\n)\n//: Synth\n\n(\nSynthDef(\\synth01, {|out = 0, gate = 1, amp = 0.1, freq = 440|\n         var env, source;\n         env = EnvGen.kr(Env.adsr, gate, doneAction: 2);\n         source = SinOsc.ar(freq, 0, amp);,\n         Out.ar(out, Pan2.ar(source*env, pan))\n}).add;\n)\n\n//: Routine\n(\n{\n\tvar data;\n\tdata = ~load.(~files.first);\n\n\t5.wait;\n\n\tdata do: { | row |\n\t\t//row.postln;\n\t\t//\t\t(dur: 0.1, degree: row[0].abs.cos.postln).play;\n\n\t\tvar addr = NetAddr(\"127.0.0.1\", 12345);\n\t\t\"TO - SYNTH\".postln;\n\n\n\t\t//\trow[1] = row[1]+1.0;\n\tSynth(\\synth01, [\\freq, row[0].abs.postln,\n\t\t\t\\amp,\nrow[1].abs.tan.postln, \\legato, 1]);\n\n\t\trow[0] = row[0]+420;\n\t\trow[1] = row[1]+512;\n\t\t\"Data-TO-OF-Fluids\".postln;\n\t\taddr.sendMsg(\"/data\", row[1].abs.asFloat.postln, row[0].abs.asFloat.postln\n\t\t);\n\n\n\t\trow[0] = row[0]+800.0;\n\t\trow[1] = row[1]+400.0;\n\t\trow[2] = row[2]+900.0;\n\t\t\t\t\"Vertex-TO-OF-3D-Model\".postln;\n\n\t\taddr.sendMsg(\"/vertex\", row[0].abs.asFloat.postln,\n\t\trow[1].abs.asFloat.postln,\n\trow[2].abs.asFloat.neg.postln, 1.0.rand, 1.0.rand, 1.0.rand);\n\n\t\t0.07.wait;//70 miliseconds\n\n\t\t}\n}.fork;\n)\n\n</pre>\n</div>\n</div>\n\n<div id=\"outline-container-org313ac84\" class=\"outline-2\">\n<h2 id=\"org313ac84\">Listening to data</h2>\n<div class=\"outline-text-2\" id=\"text-org313ac84\">\n<p>\nListen here a magnetic storm:\n</p>\n\n\n<p>\n<a href=\"https://youtu.be/FMLO1qVhWio\">Magnetic storm sonification sample</a>\n</p>\n\n<p>\n<b>Real-time sonification</b>\n</p>\n\n<p>\n<a href=\"https://www.youtube.com/watch?v=0l6I8398E6s\">Magnetic Storm Sonification</a>\n</p>\n\n<p>\n\"Magnetic Storm Sonification - Live at Megaron: Banqueting Hall - 11th Audiovisual Arts Festival May 21/17 - AKOYSMATA.\nData: NOA's ENIGMA.\nSee more about Storm Trio here: <a href=\"http://users.ionio.gr/~amlists/akousm\">http://users.ionio.gr/~amlists/akousm</a>&#x2026;\"\n</p>\n</div>\n</div>\n</div>\n"}
